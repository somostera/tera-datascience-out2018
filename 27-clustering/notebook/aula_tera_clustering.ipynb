{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERA - Aula 27\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivos gerais de algoritmos de clustering:\n",
    "- Análise exploratória dos dados\n",
    "- Encontrar padrões e estruturas\n",
    "- Agrupar dados de forma a criar representações sumarizadas (sumarização de dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "- [Exemplo inicial](#Exemplo-Inicial)\n",
    "- [K-Means](#K-Means)\n",
    " - [Case K-Means Elo7](#Case-Cluster-Usuários-Elo7)\n",
    "- [Case Elo7 - Cluster Frete](#Case-Elo7---Clustering-de-Frete)\n",
    "- [Hierarchical Clustering](#Hierarchical-Clustering)\n",
    " - [Exercício Prático](#Exercício-prático-Hierarchical-Clustering)\n",
    "- [Case Elo7 - Motivos de Compra](#Case-Elo7---Motivos-de-Compra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo Inicial\n",
    "Análise exploratória do comportamento dos usuários do Elo7.\n",
    "\n",
    "Dataset:\n",
    "- `tempo` (float): Tempo em segundos que um usuário permanece no site.\n",
    "- `ticket` (float): Valor gasto em reais no site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports usados no curso\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta contendo os dados:\n",
    "ROOT_FOLDER = os.path.realpath('..')\n",
    "DATASET_FOLDER = os.path.join(ROOT_FOLDER,'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados\n",
    "df_user_elo7 = pd.read_csv(os.path.join(DATASET_FOLDER, 'user_patterns_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_user_elo7.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise Exploratória\n",
    "- Média\n",
    "- Covariância\n",
    "- Tendência (Regressão Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor médio\n",
    "user_elo7_mean = df_user_elo7.mean().values\n",
    "\n",
    "# Covariância\n",
    "user_elo7_cov = np.cov(df_user_elo7.values[:,0], df_user_elo7.values[:,1])\n",
    "\n",
    "# Tendência - regressão\n",
    "a, b, r, p, std_err = ss.linregress(df_user_elo7.values[:,0],df_user_elo7.values[:,1])\n",
    "f = lambda x: a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- Média: {}'.format(user_elo7_mean))\n",
    "print('- Covariância: \\n{}'.format(user_elo7_cov))\n",
    "print('- Coeficiente de correlação da regressão: {:.2f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses dados parecem interessantes, mas não são suficientes. Precisamos sempre observar os dados para tirar insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos plotar o gráfico\n",
    "df_user_elo7.plot.scatter(x='tempo',y='ticket', alpha=0.5)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Ticket (R$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Função auxiliar para plotar a elipse de confiança ###\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def get_confidence_ellipse(x, y, nstd=2):\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    w, h = 2 * nstd * np.sqrt(vals)\n",
    "    ell = Ellipse(xy=(np.mean(x), np.mean(y)),\n",
    "                  width=w, height=h,\n",
    "                  angle=theta, color='red', \n",
    "                  fill=False)\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos plotar os dados no gráfico\n",
    "df_user_elo7.plot.scatter(x='tempo',y='ticket', alpha=0.5)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Ticket (R$)')\n",
    "\n",
    "# Média\n",
    "plt.plot(user_elo7_mean[0], user_elo7_mean[1], '*r', markersize=20)\n",
    "\n",
    "# 2 desvios padrão\n",
    "ell = get_confidence_ellipse(x=df_user_elo7.values[:,0],\n",
    "                             y=df_user_elo7.values[:,1])\n",
    "ax = plt.gca()\n",
    "ax.add_patch(ell)\n",
    "\n",
    "# Tendência\n",
    "x = np.array([min(df_user_elo7.values[:,0]),max(df_user_elo7.values[:,0])])\n",
    "plt.plot(x, f(x), '--g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há algo estranho nessa análise?\n",
    "- A análise está matematicamente correta, mas talvez não seja completa;\n",
    "- Precisamos levar em consideração possíveis grupos diferentes de usuários dentro dos dados. Quantos grupos você vê? Talvez entre 2 e 4 clusters?\n",
    "\n",
    "Vamos utilizar o famoso algoritmo [KMeans](https://en.wikipedia.org/wiki/K-means_clustering) para encontrar esses clusters. Podemos utilizar a implementação do [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) para isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Importe o módulo do KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Crie uma instância do K-Means pelo sklearn\n",
    "# Teste diferentes números de clusters\n",
    "n = _\n",
    "kmeans = KMeans(n_clusters=_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos encontrar os clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_user_elo7.values\n",
    "\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df_user_elo7.values[:,0],\n",
    "            y=df_user_elo7.values[:,1],\n",
    "            c=labels.astype(np.float),\n",
    "            cmap='rainbow',\n",
    "            edgecolor='k')\n",
    "plt.title('Num clusters: {}'.format(n))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os clusters estão como esperado? Tem alguma hipótese do porque esses dados estão separados dessa forma?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A grande maioria dos algoritmos de aprendizado de máquina se baseiam na noção de distância entre pontos para encontrar relações entre os dados. Essa forma de tratar os problemas é bastante intuitiva e funciona em uma grande gama de cenários. Mas, existe um problema para calcularmos distâncias quando temos diversas variáveis com características diferentes. Para entender um pouco desse problema, note que as escalas dos eixos do gráfico anterior não são iguais. Veja como ficaria o gráfico se colocássemos os eixos com mesma escala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df_user_elo7.values[:,0],\n",
    "            y=df_user_elo7.values[:,1],\n",
    "            c=labels.astype(np.float),\n",
    "            cmap='rainbow',\n",
    "            edgecolor='k')\n",
    "plt.title('Num clusters: {}'.format(n))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por isso devemos **normalizar** os dados! Essa etapa é muito importante e deve ser sempre considerada antes de executar algum algoritmo de aprendizado de máquina que se baseia em distâncias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos normalizar os dados!\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# E agora plotamos o resultado\n",
    "n_clusters = range(2,6)\n",
    "\n",
    "X = df_user_elo7.values\n",
    "\n",
    "# Dados normalizados\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for n in n_clusters:\n",
    "    estimator = KMeans(n_clusters=n)\n",
    "    estimator.fit(X_scaled)\n",
    "    labels = estimator.labels_\n",
    "    plt.scatter(x=X_scaled[:,0],\n",
    "                y=X_scaled[:,1],\n",
    "                c=labels.astype(np.float),\n",
    "                cmap='rainbow',\n",
    "                edgecolor='k')\n",
    "    plt.title('Num clusters: {}'.format(n))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok... podemos ver que podemos encontrar algumas opções de número de clusters, mas qual é o valor ideal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escolha do número de clusters\n",
    "\n",
    "Nós temos diversos métodos para escolher o número ideal de clusters. Alguns deles estão resumidos neste [artigo](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method). O método mais utilizado, entretanto, é o método do \"cotovelo\" (*elbow method*). \n",
    "\n",
    "Mas, antes de falarmos do método do cotovelo, nós precisamos definir o que é um bom cluster. É claro que isso depende de cada caso, mas as seguintes características são desejadas para a maioria dos clusters:\n",
    "- Dados não muito dispersos -> Inércia\n",
    "- Dados dentro dos clusters possuem perfil semelhante\n",
    "- Quantidade aproximadamente uniforme de dados em cada cluster\n",
    "\n",
    "#### Inércia\n",
    "\n",
    "A inércia de um cluster é definida como a soma das distâncias quadráticas de cada ponto de um cluster ao seu respectivo centroide, somada através de todos os clusters. Quanto maior é a inércia, maior será a dispersão dos clusters. Portanto, desejamos escolher um número de clusters que nos possibilite ter uma inércia baixa. Simples, mas temos um problema... O mínimo valor de inércia que podemos obter é quando cada ponto do nosso dataset pertence ao seu próprio cluster. Portanto, precisamos escolher um balanço entre baixa inércia e baixo número de clusters. \n",
    "\n",
    "Para isso, utilizamos o gráfico de cotovelo. O eixo horizontal do gráfico representa o número de clusters utilizados e o eixo vertical representa a inércia total dos clusters. O número de clusters ideal é definido como o ponto onde o gráfico se aproxima a uma horizontal (como o ponto de encontro do braço e antebraço)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(1,8,1)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X_scaled)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual a sua opinião? Quantos clusters devemos utilizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Elo7 - Clustering de Frete\n",
    "\n",
    "Um dos problemas mais complicados do Elo7 é sua dependência dos correios. Nós sofremos muito com a falta de alternativas para dar aos nossos clientes (compradores e vendedores), já que o serviço dos correios além de caro, é também instável. \n",
    "\n",
    "Para tentar resolver esse problema, o time de Data Science do Elo7 foi chamado para tentar encontrar alguma alternativa. Após algumas conversas, nós levantamos a possibilidade de utilizarmos serviços de entrega independentes dos correios. Mas, o problema é que esses serviços necessitam de um volume grande de encomendas por ponto de coleta, o que não é o caso para a maioria dos vendedores cadastrados no Elo7. \n",
    "\n",
    "Uma possível solução seria encontrar pontos de coleta que pudessem agregar pedidos de vários vendedores e enviar de uma vez só com um desses serviços alternativos. Mas, como obtemos a localização desses pontos de coleta? Podemos aplicar um algoritmo de clustering nas rotas de frete mais frequentes!\n",
    "\n",
    "Vamos tentar analisar os dados e verificar o que conseguimos obter. O dataset a seguir contém pares de endereços de origem e destino de entregas realizadas apenas na cidade de São Paulo em um curto intervalo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_route.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar os cálculos de distância, as latitudes e longitudes dos locais já foram realizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora formar nosso vetor de features contendo as posições geográficas das nossas rotas.\n",
    "\n",
    "*Dica: Será que é necessário normalizar as features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "X = df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].values\n",
    "X_scaled = _ # ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos clusters vamos utilizar? (Obs: Podemos aplicar o método do cotovelo para descobrir.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos iniciar o algoritmo de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "n = _\n",
    "kmeans = KMeans(n_clusters=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "clusters = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise da quantidade de ítens em cada cluster é sempre uma boa prática. Clusters desbalanceados são um sinal de que os dados não foram bem separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster, count = np.unique(clusters, return_counts=True)\n",
    "for l, c in zip(cluster,count):\n",
    "    print('Cluster {}: {}'.format(l,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver os gráficos para analisar qualitativamente os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('Origem')\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax1.set_xlim((-23.4,-23.9))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Destino')\n",
    "plt.scatter(x=X[:,2],\n",
    "            y=X[:,3],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax2.set_xlim((-23.4,-23.9))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou? É possível perceber clusters bem definidos? Será que podemos utilizar esses clusters para resolver nossos problemas de frete?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exemplo Prático - Kaggle NYC Taxi Trip Duration\n",
    "\n",
    "Dados:\n",
    "- id - a unique identifier for each trip\n",
    "- vendor_id - a code indicating the provider associated with the trip record\n",
    "- pickup_datetime - date and time when the meter was engaged\n",
    "- dropoff_datetime - date and time when the meter was disengaged\n",
    "- passenger_count - the number of passengers in the vehicle (driver entered value)\n",
    "- pickup_longitude - the longitude where the meter was engaged\n",
    "- pickup_latitude - the latitude where the meter was engaged\n",
    "- dropoff_longitude - the longitude where the meter was disengaged\n",
    "- dropoff_latitude - the latitude where the meter was disengaged\n",
    "- store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n",
    "- trip_duration - duration of the trip in seconds\n",
    "\n",
    "![iris](https://cdn.civitatis.com/estados-unidos/nueva-york/galeria/thumbs/taxi-nueva-york.jpg)\n",
    "\n",
    "*Solução retirada do github de [juifa-tsai](https://github.com/juifa-tsai/NYC_Taxi_Trip_Duration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar uma análise dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi = pd.read_csv(os.path.join(DATASET_FOLDER,'nyc_trip_duration_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar diversas abordagens para analisar os dados. Vamos tentar verificar os dados de localização dos passageiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = df_taxi[['pickup_longitude','pickup_latitude', 'dropoff_longitude','dropoff_latitude']]\n",
    "df_pick = df_map[['pickup_longitude','pickup_latitude']]\n",
    "df_drop = df_map[['dropoff_longitude','dropoff_latitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_map(df, zoom=0.9):\n",
    "    cutmap = zoom/100\n",
    "\n",
    "    x = df['pickup_longitude']\n",
    "    y = df['pickup_latitude']\n",
    "    x_max, x_min = x.quantile(1-cutmap), x.quantile(cutmap)\n",
    "    y_max, y_min = y.quantile(1-cutmap), y.quantile(cutmap)\n",
    "    \n",
    "    x_plot = x[(x>x_min) & (x<x_max) & (y<y_max) & (y>y_min)]\n",
    "    y_plot = y[(x>x_min) & (x<x_max) & (y<y_max) & (y>y_min)]\n",
    "    plt.scatter(x=x_plot, y=y_plot, s=5, alpha=0.3)\n",
    "    plt.tick_params(labelsize=18)\n",
    "    plt.title('Pickup', fontsize=18 )\n",
    "    plt.xlabel('Longitude', fontsize=18)\n",
    "    plt.ylabel('Latitude',  fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "plot_map(df_taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribuição dos dados é bem interessante. Podemos verificar que existe uma concentração grande de pontos dentro da ilha de Manhattan, o que é esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como segundo passo da análise dos dados, nós podemos tentar enriquecê-los utilizando técnicas de feature engineering e clustering. Vamos explorar o segundo em seguida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O racional de utilizar clustering para análise exploratória e feature engineering é o fato de encontrar estruturas implícitas nos dados. Por exemplo, se tentássemos observar cada passageiro individualmente, talvez teríamos dificuldade em encontrar um padrão nos dados. Mas, é intuitivo pensar que passageiros semelhantes (mesma localização, horário etc) possam ser agrupados e tratados como um só. Assim, podemos tratar os dados por grupos controlados de passageiros, ao invés de cada indivíduo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar encontrar clusters nos dados de início da corrida de taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans = kmeans.fit_predict(df_pick)\n",
    "df_pick['zone']  = X_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_map_zone( df, x_name, y_name, z_name, name, zoom=0.9, cluster=None ):\n",
    "\n",
    "    x = df[x_name]\n",
    "    y = df[y_name]\n",
    "    z = df[z_name]\n",
    "\n",
    "    cutmap = zoom/100\n",
    "    x_max, x_min = x.quantile(1-cutmap), x.quantile(cutmap)\n",
    "    y_max, y_min = y.quantile(1-cutmap), y.quantile(cutmap)\n",
    "    \n",
    "    zones = np.unique(z[(x>x_min) & (x<x_max) & (y<y_max) & (y>y_min)])\n",
    "\n",
    "    #cmap = plt.get_cmap('spectral') \n",
    "    cmap = plt.get_cmap('winter') \n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(zones))]\n",
    "\n",
    "    for i, zone in enumerate(zones):       \n",
    "        plt.scatter( x=x[ (z==zone) & (x>x_min) & (x<x_max) & (y<y_max) & (y>y_min) ], \n",
    "                     y=y[ (z==zone) & (x>x_min) & (x<x_max) & (y<y_max) & (y>y_min) ], \n",
    "                     s=5, alpha=0.3, c=colors[i])\n",
    "        if cluster:\n",
    "            plt.text( cluster.cluster_centers_[zone,0], cluster.cluster_centers_[zone,1], str(zone), fontsize = 12, color='r')\n",
    "\n",
    "    plt.tick_params(labelsize=18)\n",
    "    plt.title(name, fontsize=18 )\n",
    "    plt.xlabel('Longitude', fontsize=18)\n",
    "    plt.ylabel('Latitude',  fontsize=18)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(17,15))\n",
    "draw_map_zone(df_pick, 'pickup_longitude', 'pickup_latitude', 'zone', 'Pickup', cluster=kmeans)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não por acaso, os clusters encontrados se assemelham aos bairros de Nova Iorque. Esses clusters agora podem ser utilizados de diversas formas:\n",
    "- Podemos explorar a distribuição das outras features dentro de cada um dos clusters. Assim poderemos ver o quanto cada região se diferença das outras.\n",
    "- Podemos também utilizar agora as labels obtidas pelo algoritmo de clustering como entrada de outros algoritmos de machine learning. Essa técnica é muito utilizada para melhorar a precisão dos algoritmos de regressão e classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar utilizar clustering na solução encontrada na [aula 19](https://github.com/somostera/tera-datascience-out2018/blob/master/19-decision-trees/notebooks/Gabarito%20Aula%2019%20-%20%C3%81rvores%20de%20Decis%C3%A3o.ipynb) para verificar se conseguimos aumentar a precisão do regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5,random_state=0)\n",
    "\n",
    "df_taxi_cluster = df_taxi.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=100)\n",
    "X_kmeans = kmeans.fit_predict(df_map)\n",
    "df_taxi_cluster['zone'] = X_kmeans\n",
    "\n",
    "x = df_taxi.drop(['trip_duration', 'id', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag'], axis=1)\n",
    "y = df_taxi['trip_duration']\n",
    "\n",
    "x_cluster = df_taxi_cluster.drop(['trip_duration', 'id', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag'], axis=1)\n",
    "\n",
    "reg = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cv_prediction(x,y,train_index,test_index,reg):\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    reg.fit(x_train, y_train)\n",
    "    y_pred = reg.predict(x_test)\n",
    "    return y_pred, y_test\n",
    "\n",
    "def rmsle(y_test, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_test,y_pred))\n",
    "\n",
    "rmsle_cv_default = []\n",
    "rmsle_cv_cluster = []\n",
    "for train_index, test_index in kfold.split(x,y):\n",
    "    y_pred, y_test = make_cv_prediction(x,y,train_index,test_index,reg)\n",
    "    rmsle_cv_default.append(rmsle(y_test, y_pred))\n",
    "    \n",
    "    y_pred_cluster, y_test = make_cv_prediction(x_cluster,y,train_index,test_index,reg)\n",
    "    rmsle_cv_cluster.append(rmsle(y_test, y_pred_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Default: {:.4f}'.format(np.mean(rmsle_cv_default)))\n",
    "print('Cluster: {:.4f}'.format(np.mean(rmsle_cv_cluster)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se notar um ligeiro aumento de precisão do estimador ao utilizar o cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Hierarchical Clustering\n",
    "\n",
    "Vamos agora aprender sobre outro método de clustering: [**Hierarchical Clustering**](https://en.wikipedia.org/wiki/Hierarchical_clustering). Como o nome mesmo diz, ele utiliza o conceito de *hierarquia* para construir os clusters. Existem duas principais variações do algoritmo: aglomerativo e por divisão. O primeiro é mais usado na prática. O passo a passo do algoritmo é apresentado abaixo:\n",
    "\n",
    "- Primeiro colocamos todos as observações em clusters próprios (individuais);\n",
    "- Depois, iterativamente procuramos os clusters mais próximos\\* e agrupamos eles em um novo cluster;\n",
    "- Repetimos o passo anterior até formarmos um único cluster com todas as observações.\n",
    "\n",
    "\\*Obs: A definição de distância (ou similaridade) entre clusters depende do tipo de métrica de distância (Euclidiana, Manhattan, cosseno etc) e ligação (Ward, simples, completa etc).\n",
    "\n",
    "Como podemos ver no algoritmo, o objetivo é a criação de um grande cluster que agrupe todos os dados. Nós podemos visualizar esse histórico de agrupamentos a partir de um [dendrograma](https://en.wikipedia.org/wiki/Dendrogram). A então criação de clusters mais granulares depende da região de similaridade que se deseja realizar o corte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aplicar o método de Hierarchical Clustering no dataset de usuários do Elo7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_user_elo7 = pd.read_csv(os.path.join(DATASET_FOLDER, 'user_patterns_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "X = df_user_elo7.values\n",
    "\n",
    "# Dados normalizados\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe os métodos linkage (Hierarchical Clustering) e dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn possui um método próprio para o algoritmo de [Hierarchical Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering). Entretanto, ele não nos permite visualizar facilmente o dendrograma final. Por isso, vamos utilizar a versão do scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Vamos escolher a métrica de distância:\n",
    "distance = _ # 'euclidean'|'cityblock'|'cosine'...\n",
    "# Agora o tipo de ligação\n",
    "linkage_type = _ # 'single'|'complete'|'average'|'ward'...\n",
    "\n",
    "# Vamos aplicar o método linkage\n",
    "Y = linkage(X_scaled, method=linkage_type, metric=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos visualizar o dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou? Teste outros valores de distância e tipo de ligação para verificar as diferenças nos resultados!\n",
    "\n",
    "O dendrograma nos permite verificar qual é o número de clusters que vamos escolher ao final. Além disso, podemos verificar se a distância e o tipo de ligação foram bem escolhidos.\n",
    "\n",
    "O que precisamos fazer agora é escolher o número de clusters. Podemos utilizar o mesmo método do cotovelo para esse objetivo, mas, na prática, podemos apenas visualizar qual é a região que possui maior distância entre aglutinações. Outros trabalhos ainda utilizam um coeficiente de [Correlação Cofenética](https://en.wikipedia.org/wiki/Cophenetic_correlation) para encontrar uma boa posição de corte no dendrograma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para essa tarefa nós podemos usar o método [`fcluster`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.fcluster.html) do scipy. Ele nos permite realizar um corte na árvore de clustering gerada pelo Hierarchical Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Vamos gerar os rótulos para os clustes\n",
    "num_clusters = _ # escolha o número de clusters que deseja\n",
    "labels = fcluster(Y, num_clusters ,criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_scaled[:,0],\n",
    "            y=X_scaled[:,1],\n",
    "            c=labels.astype(np.float),\n",
    "            cmap='rainbow',\n",
    "            edgecolor='k')\n",
    "plt.title('Num clusters: {}'.format(num_clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Elo7 - Clustering de Frete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar aplicar o mesmo algoritmo para o problema de cluster de frete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_route.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].values\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Vamos escolher a métrica de distância:\n",
    "distance = _\n",
    "# Agora o tipo de ligação\n",
    "linkage_type = _\n",
    "\n",
    "# Vamos aplicar o método linkage\n",
    "Y = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos visualizar o dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Vamos gerar os rótulos para os clustes\n",
    "num_clusters = _\n",
    "labels = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('Origem')\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax1.set_xlim((-23.4,-23.9))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Destino')\n",
    "plt.scatter(x=X[:,2],\n",
    "            y=X[:,3],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax2.set_xlim((-23.4,-23.9))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Motivos de Compra\n",
    "\n",
    "Os compradores do Elo7 são incentivados a indicar o motivo da compra de determinado produto no seu marketplace. Esses motivos nos ajudam a entender melhor o **momento** de compra do usuário. O dataset apresentado a seguir contém um subset desses motivos de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason = pd.read_csv(os.path.join(DATASET_FOLDER, 'purchase_reason_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_reason.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem muitos tipos possíveis de motivos de compra, mas será que nós podemos encontrar algum padrão neles? Me parece um problema clássico de **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o Tf-Idf para criar o embedding dos motivos de compra e o K-Means para encontrar clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=5000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria a matriz de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_reason['reason'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como escolher o número de clusters? Vamos utilizar o gráfico de inércias. (Obs: outra possibilidade é avaliar o [\"silhouette score\"](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(10,200,20)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa o K-Means com a quantidade de clusters que escolhemos a partir do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "n = _\n",
    "kmeans = KMeans(n_clusters=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treine o modelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontra os clusters para cada motivo de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "labels = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora visualizar os clusters criados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'reason': df_reason['reason'], 'labels': labels})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar a distribuição de motivos em cada cluster. Quanto mais desbalanceado, pior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar alguns exemplos de clusters gerados pelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(50):\n",
    "    idx_labels = df[df['labels']==idx]['reason'].unique()\n",
    "    print('- Cluster {}:'.format(idx + 1))\n",
    "    for i in np.random.choice(idx_labels, min(len(idx_labels), 10), replace=False):\n",
    "        print(' '*5, i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é o resultado dos clusters gerados? Podemos avançar um pouco e verificar se existe alguma relação de hierarquia entre os motivos de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "# TODO\n",
    "Y = linkage(_, method=_, metric=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém aleatoriamente um dos motivos para representar o cluster\n",
    "titles = df.groupby('labels').apply(lambda x: np.random.choice(list(x['reason'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plota o dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=titles.values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual foi o resultado? O que você faria para melhorar o resultado obtido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Subcategorias Automáticas\n",
    "\n",
    "Vamos para mais um case real do Elo7!\n",
    "\n",
    "Esse case é um dos trabalhos mais recentes do time de Data Science do Elo7. De fato, é um trabalho ainda em aberto e qualquer sugestão de melhorias é bem vinda! =)\n",
    "\n",
    "- O problema:\n",
    "O Elo7 possui uma árvore de categorias dividida em N1 e N2. O primeiro nível (N1) contém as categorias \"alto nível\" do site. São as categorias mais genéricas do marketplace- ou, pelo menos, é assim gostaríamos que fosse. As categorias N2, ou subcategorias, são as possíveis extensões dos nós das categorias N1. Podemos perceber que a árvore é extremamente limitada e isso é um problema grave não só para os compradores, que não conseguem navegar nas nossas categorias, mas também para os vendedores, que não conseguem categorizar bem seus produtos. A solução para esse problema seria uma árvore de categorias com maior \"granularidade\", ou seja, que consiga expandir além dos 2 níveis e ter mais subcategorias.\n",
    "\n",
    "- O que o time de Data Science tem a ver com essa história? \n",
    "\n",
    "Bom, gerar uma nova árvore de categoria pode ser uma tarefa bastante monótona e cansativa. Provavelmente deve haver algum jeito de encontrar bons agrupamentos de produtos que pudessem servir como uma nova subcategoria. Talvez algum método de clustering que utilize como features o conteúdo dos produtos pode gerar algum resultado interessante.\n",
    "\n",
    "- O experimento:\n",
    "\n",
    "O dataset a seguir possui um subconjunto de produtos que foram categorizados na categoria N1 \"Casamento\". Escolhemos esse conjunto de dados para iniciar nossos trabalhos, porque assim temos mais controle sobre nossos resultados. E, também, porque é uma das categorias mais importantes do marketplace.\n",
    "\n",
    "Para essa tarefa, vamos utilizar apenas o título e uma parte da descrição do produto (aprox. 140 caracteres) como features de entrada.\n",
    "\n",
    "Vamos analisar os dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = pd.read_csv(os.path.join(DATASET_FOLDER, 'subcategory_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma coluna com as features que vamos incluir no nosso modelo de aprendizagem.\n",
    "Esse vetor de features será o título + descrição do produto. Para compensar a quantidade de palavras do título em relação a descrição, vamos repetir o título duas vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['title_desc'] = (df_cat['title'] + ' ')*2 + df_cat['short_description']\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tente encontrar as subcategorias dos produtos da categoria \"casamento\". Lembre-se de que não queremos apenas aumentar o número de subcategorias do segundo nível (N2), mas também aumentar a profundidade da nossa árvore de categorias (N3, N4 ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar nossa matriz de features, nós vamos utilizar o Tf-Idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=10000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_cat['title_desc'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, precisamos definir o número de clusters. Podemos utilizar o método do gráfico de inércias.\n",
    "\n",
    "(Obs: O cálculo pode levar muito tempo para ser executado. Assuma que o valor escolhido é 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(10,100,10)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "kmeans = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "labels = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title': df_cat['title'], 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(df['labels'].unique())):\n",
    "    idx_labels = df[df['labels']==idx]['title'].unique()\n",
    "    print('- Cluster {}:'.format(idx + 1))\n",
    "    for i in np.random.choice(idx_labels, min(len(idx_labels), 10), replace=False):\n",
    "        print(' '*5, i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "Y = linkage(_, method=_, metric=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df.groupby('labels').apply(lambda x: np.random.choice(list(x['title'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=titles.values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
